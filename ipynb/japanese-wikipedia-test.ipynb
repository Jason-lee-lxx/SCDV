{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf ... http://catindog.hatenablog.com/entry/2017/02/15/222915\n",
    "# word2vec model ... https://github.com/shiroyagicorp/japanese-word2vec-model-builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import time\n",
    "from numpy import float32\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pickle\n",
    "from math import *\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 200     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Word2Vec model.\n",
    "model = Word2Vec.load('../japanese-dataset/data/word2vec.gensim.model')\n",
    "# Get wordvectors for all words in vocabulary.\n",
    "word_vectors = model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain word2vec t-SNE Visualization\n",
    "word2vec_model=model\n",
    " \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "skip=0\n",
    "limit=241 \n",
    "\n",
    "vocab = word2vec_model.wv.vocab\n",
    "emb_tuple = tuple([word2vec_model[v] for v in vocab])\n",
    "X = np.vstack(emb_tuple)\n",
    " \n",
    "model = TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "model.fit_transform(X) \n",
    "\n",
    "plt.figure(figsize=(40,40))\n",
    "plt.scatter(model.embedding_[skip:limit, 0], model.embedding_[skip:limit, 1])\n",
    " \n",
    "count = 0\n",
    "for label, x, y in zip(vocab, model.embedding_[:, 0], model.embedding_[:, 1]):\n",
    "    count +=1\n",
    "    if(count<skip):continue\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    if(count==limit):break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_GMM(num_clusters, word_vectors):\n",
    "    # Initalize a GMM object and use it for clustering.\n",
    "    clf =  GaussianMixture(n_components=num_clusters,\n",
    "                    covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
    "    # Get cluster assignments.\n",
    "    clf.fit(word_vectors)\n",
    "    idx = clf.predict(word_vectors)\n",
    "    print (\"Clustering Done...\")\n",
    "    # Get probabilities of cluster assignments.\n",
    "    idx_proba = clf.predict_proba(word_vectors)\n",
    "#   # Dump cluster assignments and probability of cluster assignments. \n",
    "#   joblib.dump(idx, 'gmm_latestclusmodel_len2alldata.pkl')\n",
    "#   print (\"Cluster Assignments Saved...\")\n",
    "\n",
    "#   joblib.dump(idx_proba, 'gmm_prob_latestclusmodel_len2alldata.pkl')\n",
    "#   print (\"Probabilities of Cluster Assignments Saved...\")\n",
    "    return idx, idx_proba\n",
    "\n",
    "def read_GMM(idx_name, idx_proba_name):\n",
    "    # Loads cluster assignments and probability of cluster assignments. \n",
    "    idx = joblib.load(idx_name)\n",
    "    idx_proba = joblib.load(idx_proba_name)\n",
    "    print (\"Cluster Model Loaded...\")\n",
    "    return (idx, idx_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set number of clusters.\n",
    "num_clusters = 60\n",
    "# Uncomment below line for creating new clusters.\n",
    "idx, idx_proba = cluster_GMM(num_clusters, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "# Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to\n",
    "# list of probabilities of cluster assignments.\n",
    "word_centroid_prob_map = dict(zip( model.wv.index2word, idx_proba ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idf_dict = open('../japanese-dataset/data/words_idf.json', 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
