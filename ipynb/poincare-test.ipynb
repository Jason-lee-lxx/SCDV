{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "dirlist = [\"dokujo-tsushin\",\"it-life-hack\",\"kaden-channel\",\"livedoor-homme\",\n",
    "           \"movie-enter\",\"peachy\",\"smax\",\"sports-watch\",\"topic-news\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"class\",\"news\"])\n",
    "for i in tqdm(dirlist):\n",
    "    path = \"../japanese-dataset/livedoor-news-corpus/\"+i+\"/*.txt\"\n",
    "    files = glob.glob(path)\n",
    "    files.pop()\n",
    "    for j in tqdm(files):\n",
    "        f = open(j)\n",
    "        data = f.read() \n",
    "        f.close()\n",
    "        t = pd.Series([i,\"\".join(data.split(\"\\n\")[3:])],index = df.columns)\n",
    "        df  = df.append(t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read poincare embedding\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import MeCab\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "import re\n",
    " \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poincare = pd.read_csv(\"../japanese-dataset/livedoor-news-corpus/poincare/embeddings.tsv\",delimiter=\"\\t\",header=None)\n",
    "poincare.drop(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poincare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create poincare embedding dict\n",
    "poincare_vectors = {}\n",
    "for key,row in tqdm(poincare.iterrows()):\n",
    "    vectors = np.zeros(len(poincare.columns)-1,dtype=\"float32\" )\n",
    "    for j in range(len(vectors)):\n",
    "        vectors[j] = row[j+1]\n",
    "    poincare_vectors[row[0]] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(poincare_vectors.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a += \"hoge\"\n",
    "a += \"sage\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for gensim keyedvectors\n",
    "file = open('../japanese-dataset/livedoor-news-corpus/model/temp.txt', 'w')\n",
    "string_list = [str(len(poincare_vectors)),str(len(list(poincare_vectors.values())[0]))]\n",
    "string_list.append(\"\\n\")\n",
    "file.writelines(\" \".join(string_list))\n",
    "for key,value in poincare_vectors.items():\n",
    "    string_list = []\n",
    "    string_list.append(key)\n",
    "    for i in value:\n",
    "        string_list.append(str(i))\n",
    "    string_list.append(\"\\n\")\n",
    "    file.writelines(\" \".join(string_list))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "poincare_vec = KeyedVectors.load_word2vec_format(\"../japanese-dataset/livedoor-news-corpus/model/temp.txt\",binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poincare_vec.most_similar(\"HDMI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poincare_vec.save(\"../japanese-dataset/livedoor-news-corpus/model/vector-response-test/poincare_vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(poincare_vectors,open(\"../japanese-dataset/livedoor-news-corpus/model/poincare_vectors.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualization poincare embeddings\n",
    "skip=0\n",
    "limit=241 \n",
    "\n",
    "X = np.vstack(poincare_vectors.values())\n",
    " \n",
    "tsne_model = TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "tsne_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(poincare_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot') \n",
    "plainw2v = pd.DataFrame(tsne_model.embedding_[0:5000, 0],columns = [\"x\"])\n",
    "plainw2v[\"y\"] = pd.DataFrame(tsne_model.embedding_[0:5000, 1])\n",
    "plainw2v[\"word\"] = list(vocab)[0:5000]\n",
    "plainw2v.plot.scatter(x=\"x\",y=\"y\",figsize=(8, 6),s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# plotly.offline.init_notebook_mode(connected=False)\n",
    "# # Create a trace\n",
    "# trace = go.Scatter(\n",
    "#     x =tsne_model.embedding_[0:5000, 0],\n",
    "#     y = tsne_model.embedding_[0:5000, 1],       \n",
    "#     text=list(vocab)[0:5000],\n",
    "#     mode = 'markers+text'\n",
    "# )\n",
    "\n",
    "# data = [trace]\n",
    "\n",
    "# # Plot and embed in ipython notebook!\n",
    "# py.plot(data, filename = 'poincare-embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create gwbowv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "import pickle\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def drange(start, stop, step):\n",
    "    r = start\n",
    "    while r < stop:\n",
    "        yield r\n",
    "        r += step\n",
    "\n",
    "def cluster_GMM(num_clusters, word_vectors):\n",
    "    # Initalize a GMM object and use it for clustering.\n",
    "    clf =  GaussianMixture(n_components=num_clusters,\n",
    "                    covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
    "    # Get cluster assignments.\n",
    "    clf.fit(word_vectors)\n",
    "    idx = clf.predict(word_vectors)\n",
    "    print (\"Clustering Done...\", time.time()-start, \"seconds\")\n",
    "    # Get probabilities of cluster assignments.\n",
    "    idx_proba = clf.predict_proba(word_vectors)\n",
    "    # Dump cluster assignments and probability of cluster assignments. \n",
    "    pickle.dump(idx, open('../japanese-dataset/livedoor-news-corpus/model/gmm_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
    "    print (\"Cluster Assignments Saved...\")\n",
    "\n",
    "    pickle.dump(idx_proba,open( '../japanese-dataset/livedoor-news-corpus/model/gmm_prob_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
    "    print (\"Probabilities of Cluster Assignments Saved...\")\n",
    "    return (idx, idx_proba)\n",
    "\n",
    "def read_GMM(idx_name, idx_proba_name):\n",
    "    # Loads cluster assignments and probability of cluster assignments. \n",
    "    idx = pickle.load(open('../japanese-dataset/livedoor-news-corpus/model/gmm_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
    "    idx_proba = pickle.load(open( '../japanese-dataset/livedoor-news-corpus/model/gmm_prob_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
    "    print (\"Cluster Model Loaded...\")\n",
    "    return (idx, idx_proba)\n",
    "\n",
    "def get_probability_word_vectors_from_dict(featurenames, vectors,word_centroid_map, num_clusters, word_idf_dict):\n",
    "    # This function computes probability word-cluster vectors\n",
    "    prob_wordvecs = {}\n",
    "    for word in word_centroid_map:\n",
    "        prob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
    "        for index in range(0, num_clusters):\n",
    "            try:\n",
    "                prob_wordvecs[word][index*num_features:(index+1)*num_features] = vectors[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # prob_wordvecs_idf_len2alldata = {}\n",
    "    # i = 0\n",
    "    # for word in featurenames:\n",
    "    #     i += 1\n",
    "    #     if word in word_centroid_map:    \n",
    "    #         prob_wordvecs_idf_len2alldata[word] = {}\n",
    "    #         for index in range(0, num_clusters):\n",
    "    #                 prob_wordvecs_idf_len2alldata[word][index] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word] \n",
    "\n",
    "    \n",
    "\n",
    "    # for word in prob_wordvecs_idf_len2alldata.keys():\n",
    "    #     prob_wordvecs[word] = prob_wordvecs_idf_len2alldata[word][0]\n",
    "    #     for index in prob_wordvecs_idf_len2alldata[word].keys():\n",
    "    #         if index==0:\n",
    "    #             continue\n",
    "    #         prob_wordvecs[word] = np.concatenate((prob_wordvecs[word], prob_wordvecs_idf_len2alldata[word][index]))\n",
    "    return prob_wordvecs\n",
    "\n",
    "def create_cluster_vector_and_gwbowv(prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
    "    # This function computes SDV feature vectors.\n",
    "    bag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
    "    global min_no\n",
    "    global max_no\n",
    "\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            temp = word_centroid_map[word]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        bag_of_centroids += prob_wordvecs[word]\n",
    "\n",
    "    norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "    if(norm!=0):\n",
    "        bag_of_centroids /= norm\n",
    "\n",
    "    # To make feature vector sparse, make note of minimum and maximum values.\n",
    "    if train:\n",
    "        min_no += min(bag_of_centroids)\n",
    "        max_no += max(bag_of_centroids)\n",
    "\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GMM clustering\n",
    "start = time.time()\n",
    "num_features = 200     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Load train data.\n",
    "train,test = train_test_split(df,test_size=0.3,random_state=40)\n",
    "all = df\n",
    "\n",
    "# Set number of clusters.\n",
    "num_clusters = 60\n",
    "# Uncomment below line for creating new clusters.\n",
    "idx, idx_proba = cluster_GMM(num_clusters, list(poincare_vectors.values()))\n",
    "\n",
    "# Uncomment below lines for loading saved cluster assignments and probabaility of cluster assignments.\n",
    "# idx_name = \"gmm_latestclusmodel_len2alldata.pkl\"\n",
    "# idx_proba_name = \"gmm_prob_latestclusmodel_len2alldata.pkl\"\n",
    "# idx, idx_proba = read_GMM(idx_name, idx_proba_name)\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "word_centroid_map = dict(zip( list(poincare_vectors.keys()), idx ))\n",
    "# Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to\n",
    "# list of probabilities of cluster assignments.\n",
    "word_centroid_prob_map = dict(zip( list(poincare_vectors.keys()), idx_proba ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer =  MeCab.Tagger(\"-Owakati\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing tf-idf values.\n",
    "traindata = []\n",
    "for review in all[\"news\"]:\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = list(filter((\"\").__ne__, h))\n",
    "    traindata.append(\" \".join(h))\n",
    "\n",
    "tfv = TfidfVectorizer(dtype=np.float32)\n",
    "tfidfmatrix_traindata = tfv.fit_transform(traindata)\n",
    "featurenames = tfv.get_feature_names()\n",
    "idf = tfv._tfidf.idf_\n",
    "\n",
    "# Creating a dictionary with word mapped to its idf value \n",
    "print (\"Creating word-idf dictionary for Training set...\")\n",
    "\n",
    "word_idf_dict = {}\n",
    "for pair in zip(featurenames, idf):\n",
    "    word_idf_dict[pair[0]] = pair[1]\n",
    "    \n",
    "# Pre-computing probability word-cluster vectors.\n",
    "prob_wordvecs = get_probability_word_vectors_from_dict(featurenames, poincare_vectors,word_centroid_map, num_clusters, word_idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(prob_wordvecs,open(\"../japanese-dataset/livedoor-news-corpus/model/prob_wordvecs_poincare.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for gensim keyedvectors\n",
    "file = open('../japanese-dataset/livedoor-news-corpus/model/temp.txt', 'w')\n",
    "string_list = [str(len(prob_wordvecs)),str(len(list(prob_wordvecs.values())[0]))]\n",
    "string_list.append(\"\\n\")\n",
    "file.writelines(\" \".join(string_list))\n",
    "for key,value in tqdm(prob_wordvecs.items()):\n",
    "    string_list = []\n",
    "    string_list.append(key)\n",
    "    for i in value:\n",
    "        string_list.append(str(i))\n",
    "    string_list.append(\"\\n\")\n",
    "    file.writelines(\" \".join(string_list))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poincare_vec_weighted = KeyedVectors.load_word2vec_format(\"../japanese-dataset/livedoor-news-corpus/model/temp.txt\",binary=False)\n",
    "poincare_vec_weighted.save(\"../japanese-dataset/livedoor-news-corpus/model/vector-response-test/poincare_vec_weighted.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plot modified word vector representation\n",
    "skip=0\n",
    "limit=500\n",
    "\n",
    "vocab = list(prob_wordvecs.keys())\n",
    "tsne_target = []\n",
    "for i in range(limit):\n",
    "    tsne_target.append(prob_wordvecs[vocab[i]])\n",
    "X = np.vstack(tsne_target)\n",
    " \n",
    "tsne_model_scdv = TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "tsne_model_scdv.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdvw2v = pd.DataFrame(tsne_model_scdv.embedding_[skip:limit, 0],columns = [\"x\"])\n",
    "scdvw2v[\"y\"] = pd.DataFrame(tsne_model_scdv.embedding_[skip:limit, 1])\n",
    "scdvw2v[\"word\"] = list(vocab)[skip:limit]\n",
    "scdvw2v.plot.scatter(x=\"x\",y=\"y\",figsize=(8, 6),s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gwbowv is a matrix which contains normalised document vectors.\n",
    "gwbowv = np.zeros( (train[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "min_no = 0\n",
    "max_no = 0\n",
    "for review in train[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    gwbowv[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters, train=True)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Train News Covered : \",counter)\n",
    "\n",
    "gwbowv_name = \"SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "\n",
    "gwbowv_test = np.zeros( (test[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in test[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    gwbowv_test[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Test News Covered : \",counter)\n",
    "\n",
    "test_gwbowv_name = \"TEST_SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Making sparse...\")\n",
    "# Set the threshold percentage for making it sparse. \n",
    "percentage = 0.04\n",
    "min_no = min_no*1.0/len(train[\"news\"])\n",
    "max_no = max_no*1.0/len(train[\"news\"])\n",
    "print (\"Average min: \", min_no)\n",
    "print (\"Average max: \", max_no)\n",
    "thres = (abs(max_no) + abs(min_no))/2\n",
    "thres = thres*percentage\n",
    "\n",
    "# Make values of matrices which are less than threshold to zero.\n",
    "temp = abs(gwbowv) < thres\n",
    "gwbowv[temp] = 0\n",
    "\n",
    "temp = abs(gwbowv_test) < thres\n",
    "gwbowv_test[temp] = 0\n",
    "\n",
    "#saving gwbowv train and test matrices\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/poincare/\"+gwbowv_name, gwbowv)\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/poincare/\"+test_gwbowv_name, gwbowv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test lgb\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
    "clf.fit(gwbowv, train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(gwbowv_test)\n",
    "print (\"Report\")\n",
    "print (classification_report(Y_true, Y_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(gwbowv_test,test[\"class\"]))\n",
    "print (\"Time taken:\", time.time() - start, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
