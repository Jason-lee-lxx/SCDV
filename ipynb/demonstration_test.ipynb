{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../20news\")\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "from numpy import float32\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pickle\n",
    "from math import *\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 200     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
    "# Load the trained Word2Vec model.\n",
    "model = Word2Vec.load(os.path.join(\"../20news/\",model_name))\n",
    "# Get wordvectors for all words in vocabulary.\n",
    "word_vectors = model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_GMM(num_clusters, word_vectors):\n",
    "\t# Initalize a GMM object and use it for clustering.\n",
    "    clf =  GaussianMixture(n_components=num_clusters,\n",
    "                    covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
    "\t# Get cluster assignments.\n",
    "    clf.fit(word_vectors)\n",
    "    idx = clf.predict(word_vectors)\n",
    "    print (\"Clustering Done...\")\n",
    "\t# Get probabilities of cluster assignments.\n",
    "    idx_proba = clf.predict_proba(word_vectors)\n",
    "# \t# Dump cluster assignments and probability of cluster assignments. \n",
    "# \tjoblib.dump(idx, 'gmm_latestclusmodel_len2alldata.pkl')\n",
    "# \tprint (\"Cluster Assignments Saved...\")\n",
    "\n",
    "# \tjoblib.dump(idx_proba, 'gmm_prob_latestclusmodel_len2alldata.pkl')\n",
    "# \tprint (\"Probabilities of Cluster Assignments Saved...\")\n",
    "    return idx, idx_proba\n",
    "\n",
    "def read_GMM(idx_name, idx_proba_name):\n",
    "\t# Loads cluster assignments and probability of cluster assignments. \n",
    "\tidx = joblib.load(idx_name)\n",
    "\tidx_proba = joblib.load(idx_proba_name)\n",
    "\tprint (\"Cluster Model Loaded...\")\n",
    "\treturn (idx, idx_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set number of clusters.\n",
    "num_clusters = 60\n",
    "# Uncomment below line for creating new clusters.\n",
    "idx, idx_proba = cluster_GMM(num_clusters, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "# Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to\n",
    "# list of probabilities of cluster assignments.\n",
    "word_centroid_prob_map = dict(zip( model.wv.index2word, idx_proba ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train data.\n",
    "train = pd.read_csv( '../20news/data/train_v2.tsv', header=0, delimiter=\"\\t\")\n",
    "# Load test data.\n",
    "test = pd.read_csv( '../20news/data/test_v2.tsv', header=0, delimiter=\"\\t\")\n",
    "all = pd.read_csv( '../20news/data/all_v2.tsv', header=0, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing tf-idf values.\n",
    "traindata = []\n",
    "for i in range(len(all[\"news\"])):\n",
    "    traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(all[\"news\"][i], True)))\n",
    "\n",
    "tfv = TfidfVectorizer(strip_accents='unicode',dtype=np.float32)\n",
    "tfidfmatrix_traindata = tfv.fit_transform(traindata)\n",
    "featurenames = tfv.get_feature_names()\n",
    "idf = tfv._tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary with word mapped to its idf value \n",
    "print (\"Creating word-idf dictionary for Training set...\")\n",
    "\n",
    "word_idf_dict = {}\n",
    "for pair in zip(featurenames, idf):\n",
    "    word_idf_dict[pair[0]] = pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict):\n",
    "\t# This function computes probability word-cluster vectors.\n",
    "\tprob_wordvecs = {}\n",
    "\tfor word in word_centroid_map:\n",
    "\t\tprob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
    "\t\tfor index in range(0, num_clusters):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprob_wordvecs[word][index*num_features:(index+1)*num_features] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t# prob_wordvecs_idf_len2alldata = {}\n",
    "\t# i = 0\n",
    "\t# for word in featurenames:\n",
    "\t# \ti += 1\n",
    "\t# \tif word in word_centroid_map:\t\n",
    "\t# \t\tprob_wordvecs_idf_len2alldata[word] = {}\n",
    "\t# \t\tfor index in range(0, num_clusters):\n",
    "\t# \t\t\t\tprob_wordvecs_idf_len2alldata[word][index] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word] \n",
    "\n",
    "\t\n",
    "\n",
    "\t# for word in prob_wordvecs_idf_len2alldata.keys():\n",
    "\t# \tprob_wordvecs[word] = prob_wordvecs_idf_len2alldata[word][0]\n",
    "\t# \tfor index in prob_wordvecs_idf_len2alldata[word].keys():\n",
    "\t# \t\tif index==0:\n",
    "\t# \t\t\tcontinue\n",
    "\t# \t\tprob_wordvecs[word] = np.concatenate((prob_wordvecs[word], prob_wordvecs_idf_len2alldata[word][index]))\n",
    "\treturn prob_wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-computing probability word-cluster vectors.\n",
    "prob_wordvecs = get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gwbowv = np.zeros( (train[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cluster_vector_and_gwbowv(prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
    "\t# This function computes SDV feature vectors.\n",
    "    # num_clusters = num_centroids\n",
    "    # num_features = dimension\n",
    "\tbag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
    "\tglobal min_no\n",
    "\tglobal max_no\n",
    "\n",
    "\tfor word in wordlist:\n",
    "\t\ttry:\n",
    "            # return cluster\n",
    "\t\t\ttemp = word_centroid_map[word]\n",
    "\t\texcept:\n",
    "\t\t\tcontinue\n",
    "        \n",
    "        # plus probability\n",
    "\t\tbag_of_centroids += prob_wordvecs[word]\n",
    "\n",
    "\tnorm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "\tif(norm!=0):\n",
    "\t\tbag_of_centroids /= norm\n",
    "\n",
    "\t# To make feature vector sparse, make note of minimum and maximum values.\n",
    "\tif train:\n",
    "\t\tmin_no += min(bag_of_centroids)\n",
    "\t\tmax_no += max(bag_of_centroids)\n",
    "\n",
    "\treturn bag_of_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "min_no = 0\n",
    "max_no = 0\n",
    "for review in train[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    # Create each Document vector\n",
    "    words = KaggleWord2VecUtility.review_to_wordlist( review,remove_stopwords=True )\n",
    "    gwbowv[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters, train=True)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Train News Covered : \",counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "gwbowv_test = np.zeros( (test[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "for review in test[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    words = KaggleWord2VecUtility.review_to_wordlist( review, \\\n",
    "        remove_stopwords=True )\n",
    "    gwbowv_test[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Test News Covered : \",counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Making sparse...\")\n",
    "# Set the threshold percentage for making it sparse. \n",
    "percentage = 0.04\n",
    "min_no = min_no*1.0/len(train[\"news\"])\n",
    "max_no = max_no*1.0/len(train[\"news\"])\n",
    "print (\"Average min: \", min_no)\n",
    "print (\"Average max: \", max_no)\n",
    "thres = (abs(max_no) + abs(min_no))/2\n",
    "thres = thres*percentage\n",
    "\n",
    "# Make values of matrices which are less than threshold to zero.\n",
    "temp = abs(gwbowv) < thres\n",
    "gwbowv[temp] = 0\n",
    "\n",
    "temp = abs(gwbowv_test) < thres\n",
    "gwbowv_test[temp] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
